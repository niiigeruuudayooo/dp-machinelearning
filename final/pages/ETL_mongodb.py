# -*- coding: utf-8 -*-
"""ETL_mongodb.py (Corrected Column Handling)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFNbMteEB4vOUAW82wa4CQ42ijk9hFCY
"""

# etl_to_mongo.py (Corrected Column Handling v2)
# Reads CSV data, cleans it, and loads it into MongoDB collections.

import pandas as pd
from pymongo import MongoClient
import traceback
from datetime import datetime

# --- Configuration ---
MONGO_URI = "mongodb://localhost:27017/"  # Change if needed
DB_NAME = "nyc_health_air_quality"

# Define expected ORIGINAL column names and the TARGET names for MongoDB
# We will clean/standardize column names read from CSV first
COLLECTIONS_INFO = {
    # Collection Name: { csv_file, target_value_col, orig_date_col, orig_value_col, orig_geo_col }
    'boiler': {'file': 'Boiler_Emissions-_Total_SO2_Emissions.csv', 'target_value_col': 'boiler_emissions', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'},
    'pm25': {'file': 'Fine_particles_PM_2.5.csv', 'target_value_col': 'PM25', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'},
    'no2': {'file': 'Nitrogen_dioxide_NO2.csv', 'target_value_col': 'NO2', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'},
    'o3': {'file': 'Ozone_O3.csv', 'target_value_col': 'O3', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'},
    'asthma': {'file': 'Asthma_emergency_department_visits_due_to_PM2.5.csv', 'target_value_col': 'asthma_rate', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'}, # Assuming Start_Date for health too
    'resp': {'file': 'Respiratory_hospitalizations_due_to_PM2.5_age_20+.csv', 'target_value_col': 'respiratory_rate', 'orig_date_col': 'Start_Date', 'orig_value_col': 'Data Value', 'orig_geo_col': 'Geo Place Name'} # Assuming Start_Date for health too
}
ALLOWED_GEO_TYPES = ["Borough", "Citywide", "UHF42"]

# --- Helper Function for Date Parsing ---
def parse_date(date_str):
    """Attempts to parse dates from common formats."""
    if pd.isna(date_str): return None
    if isinstance(date_str, datetime): return date_str # Already datetime
    # Handle potential integer years (less likely now assuming Start_Date)
    if isinstance(date_str, (int, float)) and 1900 < date_str < 2100:
         return pd.to_datetime(f'{int(date_str)}-01-01', errors='coerce')
    if isinstance(date_str, str):
        try: return pd.to_datetime(date_str, errors='coerce') # Try default first
        except Exception: pass
        for fmt in ('%m/%d/%Y', '%Y-%m-%d', '%Y'): # Add more formats if needed
            try:
                dt = pd.to_datetime(date_str, format=fmt, errors='coerce')
                if not pd.isna(dt): return dt
            except ValueError: continue
    return None # Return None if all parsing fails

# --- Main ETL Function ---
def run_etl():
    try:
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        client.admin.command('ping') # Test connection
        db = client[DB_NAME]
        print(f"Connected to MongoDB. Database: {DB_NAME}")

        for coll_name, info in COLLECTIONS_INFO.items():
            print(f"\nProcessing '{coll_name}' from '{info['file']}'...")
            try:
                df = pd.read_csv(info['file'])
                print(f"  Read {len(df)} rows from {info['file']}.")
            except FileNotFoundError:
                print(f"  ERROR: File not found: {info['file']}. Skipping.")
                continue
            except Exception as e:
                print(f"  ERROR: Could not read {info['file']}: {e}. Skipping.")
                continue

            # --- Standardize column names (lowercase, underscore) ---
            original_columns = df.columns.tolist()
            df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()
            cleaned_columns = df.columns.tolist()
            # Create mapping from cleaned back to original for error messages if needed
            # col_map_clean_to_orig = {clean: orig for clean, orig in zip(cleaned_columns, original_columns)}

            # --- Identify the *cleaned* names of essential columns ---
            cleaned_geo_col = info['orig_geo_col'].replace(' ', '_').lower()
            cleaned_date_col = info['orig_date_col'].replace(' ', '_').lower()
            cleaned_value_col = info['orig_value_col'].replace(' ', '_').lower()
            target_value_col = info['target_value_col'] # The final desired name

            # --- Check if essential *cleaned* columns exist ---
            required_cleaned = [cleaned_geo_col, cleaned_date_col, cleaned_value_col]
            missing_cleaned = [col for col in required_cleaned if col not in cleaned_columns]

            if missing_cleaned:
                 print(f"  ERROR: Essential columns missing after cleaning names: {missing_cleaned}. Check CSV headers and CONFIG. Skipping '{coll_name}'.")
                 print(f"    Expected cleaned names: {required_cleaned}")
                 print(f"    Actual cleaned names found: {cleaned_columns}")
                 continue

            # --- Rename the cleaned value column to the target name ---
            if cleaned_value_col != target_value_col:
                 print(f"  Renaming value column '{cleaned_value_col}' to '{target_value_col}'")
                 df = df.rename(columns={cleaned_value_col: target_value_col})

            # --- Filter Geo Type ---
            if 'geo_type_name' in df.columns:
                initial_rows = len(df)
                df = df[df['geo_type_name'].isin(ALLOWED_GEO_TYPES)].copy()
                print(f"  Filtered by Geo Type: {initial_rows} -> {len(df)} rows.")
                if df.empty:
                    print(f"  INFO: No data remaining after filtering Geo Types. Skipping.")
                    continue

            # --- Date Handling ---
            print(f"  Parsing date column: '{cleaned_date_col}'")
            df['date'] = df[cleaned_date_col].apply(parse_date)
            rows_before_date_drop = len(df)
            df = df.dropna(subset=['date'])
            print(f"  Dropped {rows_before_date_drop - len(df)} rows with invalid dates.")

            # --- Drop rows with missing essential data ---
            essential_final_cols = ['geo_place_name', 'date', target_value_col]
            # Rename geo column *before* dropping NA
            if cleaned_geo_col != 'geo_place_name':
                 df = df.rename(columns={cleaned_geo_col: 'geo_place_name'})

            rows_before_na_drop = len(df)
            df = df.dropna(subset=essential_final_cols)
            print(f"  Dropped {rows_before_na_drop - len(df)} rows with missing geo_place_name, date, or {target_value_col}.")

            if df.empty:
                print(f"  INFO: No valid data remaining after cleaning/parsing. Skipping.")
                continue

            # --- Select Columns for Mongo ---
            cols_to_keep = ['geo_place_name', 'date', target_value_col]
            if 'geo_join_id' in df.columns: cols_to_keep.append('geo_join_id')
            if 'geo_type_name' in df.columns: cols_to_keep.append('geo_type_name')
            cols_to_keep = [col for col in cols_to_keep if col in df.columns] # Final check
            df_final = df[cols_to_keep].copy()

            records = df_final.to_dict('records')

            # --- Load to MongoDB ---
            collection = db[coll_name]
            print(f"  Dropping existing collection '{coll_name}' (if any)...")
            collection.drop()
            print(f"  Inserting {len(records)} records into '{coll_name}'...")
            if records:
                collection.insert_many(records)
                collection.create_index([("geo_place_name", 1), ("date", 1)])
                print(f"  Successfully loaded data into '{coll_name}'.")
            else:
                print(f"  No records to insert for '{coll_name}'.")

        print("\nETL process completed.")

    except Exception as e:
        print(f"\nAn error occurred during the ETL process: {e}")
        print(traceback.format_exc())
    finally:
        if 'client' in locals() and client is not None:
            try:
                client.close()
                print("MongoDB connection closed.")
            except Exception as ce:
                print(f"Error closing MongoDB connection: {ce}")

# --- Run the ETL process ---
if __name__ == "__main__":
    run_etl()