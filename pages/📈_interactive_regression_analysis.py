# -*- coding: utf-8 -*-
"""ðŸ“ˆ Interactive Regression Analysis.py (No Prediction)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GRMRgxEB8nr9n-qDqbhU1sPbhwaSRV26
"""

# pages/ðŸ“ˆ_Interactive_Regression_Analysis.py

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Removed: RandomForestRegressor, r2_score as they are not needed without prediction
import numpy as np
import os
import traceback

# --- Configuration & Caching ---
st.set_page_config(layout="wide", page_title="Interactive Regression")

# --- Constants ---
try:
    BASE_PATH = os.path.join(os.path.dirname(__file__), '..', 'data')
except NameError:
    BASE_PATH = 'data'

# --- Data Loading ---
@st.cache_data
def load_data():
    """Loads all necessary CSV files into pandas DataFrames from the 'data' folder."""
    files = {
        'boiler': os.path.join(BASE_PATH, 'Boiler_Emissions-_Total_SO2_Emissions.csv'),
        'pm25': os.path.join(BASE_PATH, 'Fine_particles_PM_2.5.csv'),
        'no2': os.path.join(BASE_PATH, 'Nitrogen_dioxide_NO2.csv'),
        'o3': os.path.join(BASE_PATH, 'Ozone_O3.csv'),
        # Health data is no longer strictly needed for this page's core function (regression)
        # but preprocessing might still use it if included. Keep loading for now.
        'asthma': os.path.join(BASE_PATH, 'Asthma_emergency_department_visits_due_to_PM2.5.csv'),
        'resp': os.path.join(BASE_PATH, 'Respiratory_hospitalizations_due_to_PM2.5_age_20+.csv')
    }
    datasets = {}
    all_loaded = True
    for name, filepath in files.items():
        try:
            datasets[name] = pd.read_csv(filepath)
            print(f"Successfully loaded: {filepath}")
        except FileNotFoundError:
            st.error(f"Error: File not found at {filepath}.")
            datasets[name] = None
            all_loaded = False
        except Exception as e:
            st.error(f"An error occurred loading {filepath}: {e}")
            datasets[name] = None
            all_loaded = False
    if not all_loaded:
        st.warning("Some data files could not be loaded.")
    # Return only pollutant dataframes needed for regression, plus health if preprocess depends on it
    return datasets.get('boiler'), datasets.get('pm25'), datasets.get('no2'), datasets.get('o3'), datasets.get('asthma'), datasets.get('resp')


# --- Data Preprocessing ---
@st.cache_data
def preprocess_data(boiler, pm25, no2, o3, asthma, resp):
    """
    Cleans, standardizes, merges, and interpolates pollutant data.
    Returns a single DataFrame suitable for regression analysis.
    """
    # Check if pollutant data is loaded (essential for regression)
    if any(df is None for df in [boiler, pm25, no2, o3]):
        st.error("Preprocessing cannot proceed because essential pollutant datasets failed to load.")
        return None

    try:
        # --- Rename and Format Dates ---
        dfs_to_process = {
            'boiler': (boiler.copy() if boiler is not None else None, 'boiler_emissions'),
            'pm25': (pm25.copy() if pm25 is not None else None, 'PM25'),
            'no2': (no2.copy() if no2 is not None else None, 'NO2'),
            'o3': (o3.copy() if o3 is not None else None, 'O3'),
             # Include health data if needed for finding common locations/dates, but don't require it
            'asthma': (asthma.copy() if asthma is not None else None, 'asthma_rate'),
            'resp': (resp.copy() if resp is not None else None, 'respiratory_rate')
        }
        processed_dfs = {}
        all_places = set()
        all_dates = set() # Using YearMonth

        # Process only available pollutant dataframes first to find common places/dates
        pollutant_keys = ['boiler', 'pm25', 'no2', 'o3']
        valid_pollutant_dfs_exist = False
        for name in pollutant_keys:
             df, value_col = dfs_to_process[name]
             if df is None:
                  processed_dfs[name] = pd.DataFrame()
                  continue # Skip if not loaded

             # --- Column Standardization (same as before) ---
             df.columns = [col.strip() for col in df.columns]
             rename_map = {}
             geo_col_name = None
             date_col_name = None
             value_col_name = None
             for col in df.columns:
                  col_lower = col.lower().replace('_', ' ').strip()
                  if col_lower == 'geo place name': geo_col_name = col
                  elif col_lower == 'start date' or col_lower == 'date': date_col_name = col
                  elif col_lower == 'data value': value_col_name = col
             if geo_col_name: rename_map[geo_col_name] = 'Geo Place Name'
             if date_col_name: rename_map[date_col_name] = 'Date_Orig'
             if value_col_name: rename_map[value_col_name] = value_col
             df = df.rename(columns=rename_map)
             # --- End Column Standardization ---

             essential_cols = ['Geo Place Name', 'Date_Orig', value_col]
             if not all(col in df.columns for col in essential_cols):
                  st.warning(f"Pollutant dataset '{name}' missing essential columns. Skipping.")
                  processed_dfs[name] = pd.DataFrame()
                  continue

             # --- Date Handling & Value Conversion (same as before) ---
             df[value_col] = pd.to_numeric(df[value_col], errors='coerce')
             df['Date_temp'] = pd.to_datetime(df['Date_Orig'], errors='coerce')
             df = df.dropna(subset=['Date_temp', value_col])
             # --- End Date Handling ---

             if df.empty:
                  processed_dfs[name] = pd.DataFrame()
                  continue

             df['YearMonth'] = df['Date_temp'].dt.to_period("M").astype(str)
             df['Year'] = df['Date_temp'].dt.year

             # Aggregate potential duplicates (mean)
             group_cols = ['Geo Place Name', 'YearMonth', 'Year', 'Date_temp']
             df[value_col] = pd.to_numeric(df[value_col], errors='coerce')
             df_cleaned = df.groupby([col for col in group_cols if col in df.columns], as_index=False)[value_col].mean()

             processed_dfs[name] = df_cleaned
             all_places.update(df_cleaned['Geo Place Name'].unique())
             all_dates.update(df_cleaned['YearMonth'].unique())
             valid_pollutant_dfs_exist = True

        if not valid_pollutant_dfs_exist:
             st.error("No valid pollutant data found after cleaning.")
             return None

        # --- Create full grid & Merge Pollutants ---
        if not all_places or not all_dates:
            st.error("Could not create a combined grid from pollutant data.")
            return None

        all_places = sorted(list(all_places))
        all_dates = sorted(list(all_dates))
        full_grid = pd.MultiIndex.from_product(
            [all_places, all_dates], names=["Geo Place Name", "YearMonth"]
        ).to_frame(index=False)

        merged_data = full_grid.copy()
        pollutant_cols = []
        for name in pollutant_keys:
             df = processed_dfs.get(name)
             if df is not None and not df.empty:
                  value_col = dfs_to_process[name][1]
                  if value_col in df.columns:
                       merge_cols = ['Geo Place Name', 'YearMonth', value_col]
                       merged_data = pd.merge(merged_data, df[merge_cols],
                                              on=["Geo Place Name", "YearMonth"], how="left")
                       if value_col not in pollutant_cols:
                            pollutant_cols.append(value_col)

        # Interpolate pollutants (same as before)
        merged_data['Date_temp'] = pd.to_datetime(merged_data['YearMonth'], errors='coerce')
        merged_data = merged_data.dropna(subset=['Date_temp'])
        merged_data = merged_data.sort_values(by=['Geo Place Name', 'Date_temp'])
        for col in pollutant_cols:
             if col in merged_data.columns:
                  merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')
                  merged_data[col] = merged_data.groupby('Geo Place Name', group_keys=False)[col].apply(lambda group: group.interpolate(method='linear', limit_direction='both'))

        # Drop rows where essential pollutants are NaN after interpolation
        final_df = merged_data.dropna(subset=pollutant_cols)
        final_df = final_df.drop(columns=['Date_temp'], errors='ignore')

        # No longer need to prepare X_model, y_asthma_model, y_resp_model
        return final_df # Return only the merged/interpolated dataframe

    except Exception as e:
        st.error(f"An error occurred during data preprocessing: {e}")
        st.error(traceback.format_exc())
        return None


# --- Load and preprocess data ---
boiler_raw, pm25_raw, no2_raw, o3_raw, asthma_raw, resp_raw = load_data()
# Only expect final_data now
final_data = preprocess_data(
    boiler_raw, pm25_raw, no2_raw, o3_raw, asthma_raw, resp_raw # Pass health data if preprocess uses it
)

# --- REMOVED Model Training Section ---

# --- Page Title ---
st.title("ðŸ“Š Interactive Regression Analysis")
st.markdown("Explore relationships between different air pollutants.")

# --- Check if data processing was successful ---
if final_data is not None and not final_data.empty:

    # --- Sidebar Filters ---
    st.sidebar.header("âš™ï¸ Filter Options")
    locations = sorted(final_data['Geo Place Name'].unique())
    if not locations:
        st.error("No locations found in the final processed data.")
        st.stop()

    default_ix = locations.index('NYC') if 'NYC' in locations else 0
    selected_location = st.sidebar.selectbox("Select Location:", options=locations, index=default_ix)

    # Define available pollutants based on columns present in final_data
    available_pollutants = [col for col in ['boiler_emissions', 'PM25', 'NO2', 'O3'] if col in final_data.columns]
    if not available_pollutants:
         st.error("No pollutant columns found in the final processed data for analysis.")
         st.stop() # Stop if no pollutants can be selected

    # --- REMOVED Health Outcome Radio Button ---

    # Filter data for the selected location (needed for regression plot)
    filtered_by_location = final_data[final_data['Geo Place Name'] == selected_location].copy()

    if filtered_by_location.empty:
         st.warning(f"No data found for the selected location: {selected_location}")
         # Stop execution if no data for the location
         st.stop()


    # --- Interactive Regression Analysis Section (Using Matplotlib/Seaborn) ---
    st.header("ðŸ“ˆ Interactive Regression Analysis")
    st.markdown(f"Comparing pollutant relationships within **{selected_location}**.")

    pollutant_display_names = {
        'boiler_emissions': 'Boiler Emissions',
        'PM25': 'PM2.5',
        'NO2': 'NOâ‚‚',
        'O3': 'Oâ‚ƒ'
    }

    col1, col2 = st.columns(2)
    with col1:
        x_pollutant = st.selectbox(
            "Select X-axis Pollutant:", options=available_pollutants,
            format_func=lambda x: pollutant_display_names.get(x, x), index=0, key="reg_x_select_page_mpl"
        )
    with col2:
        y_pollutant = st.selectbox(
            "Select Y-axis Pollutant:", options=available_pollutants,
            format_func=lambda x: pollutant_display_names.get(x, x),
            index=min(1, len(available_pollutants)-1) if len(available_pollutants) > 1 else 0,
            key="reg_y_select_page_mpl"
        )

    if x_pollutant == y_pollutant and len(available_pollutants)>1:
        st.warning("Please select two **different** pollutants.")
    elif x_pollutant != y_pollutant or len(available_pollutants)<=1:
        if x_pollutant in filtered_by_location.columns and y_pollutant in filtered_by_location.columns:
            reg_data = filtered_by_location[[x_pollutant, y_pollutant]].apply(pd.to_numeric, errors='coerce').dropna()

            if not reg_data.empty and len(reg_data) > 1:
                try:
                    fig_reg, ax_reg = plt.subplots(figsize=(10, 6))
                    sns.regplot(
                        data=reg_data, x=x_pollutant, y=y_pollutant,
                        scatter_kws={'s': 50, 'alpha': 0.6},
                        line_kws={'color': 'red'},
                        ax=ax_reg
                    )
                    ax_reg.set_xlabel(pollutant_display_names.get(x_pollutant, x_pollutant))
                    ax_reg.set_ylabel(pollutant_display_names.get(y_pollutant, y_pollutant))
                    ax_reg.set_title(f"Regression: {pollutant_display_names.get(y_pollutant, y_pollutant)} vs {pollutant_display_names.get(x_pollutant, x_pollutant)}")
                    ax_reg.grid(True, alpha=0.3)
                    st.pyplot(fig_reg)

                    if reg_data[x_pollutant].nunique() > 1 and reg_data[y_pollutant].nunique() > 1:
                         corr = reg_data.corr().iloc[0, 1]
                         st.info(f"Correlation coefficient (r): {corr:.3f}")
                    else:
                         st.info("Correlation coefficient cannot be calculated (data may be constant).")

                except Exception as e:
                    st.error(f"Could not generate regression plot: {e}")
                    st.error(traceback.format_exc())
            else:
                st.warning(f"Insufficient overlapping numeric data for regression between '{pollutant_display_names.get(x_pollutant, x_pollutant)}' and '{pollutant_display_names.get(y_pollutant, y_pollutant)}' in {selected_location}.")
        else:
             st.warning(f"One or both selected pollutants ('{x_pollutant}', '{y_pollutant}') not found in the processed data for {selected_location}.")

    # --- REMOVED Health Outcome Prediction Section ---
    # st.markdown("---") # Separator no longer needed unless adding more sections

else:
    # Message if data loading/processing failed at the start
    st.error("Data could not be loaded or processed successfully. Cannot display analysis.")
    st.info("Please ensure all required pollutant CSV files are present in the 'data' folder and have the expected columns/formats.")

# --- Footer ---
st.sidebar.markdown("---")
st.sidebar.info("Analysis based on NYC Open Data.")